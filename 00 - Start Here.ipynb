{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Welcome to the Kedro-Ibis tutorial!\n",
        "\n",
        "The side bar on the left ðŸ‘ˆ shows the tutorial files in the Explorer.\n",
        "\n",
        "For convenience, they're also linked below: ðŸ‘‡\n",
        "\n",
        "1. [Getting Started](./01%20-%20Getting%20Started.ipynb)\n",
        "1. [Ibis and the Python Ecosystem](./02%20-%20Ibis%20and%20the%20Python%20Ecosystem.ipynb)\n",
        "1. [Switching Backends](./03%20-%20Switching%20Backends.ipynb)\n",
        "1. [Playing with PyPI](./04%20-%20Playing%20with%20PyPI.ipynb)\n",
        "\n",
        "First, let's download the [nycflights13 data](https://github.com/hadley/nycflights13); we'll use this later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import tempfile\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import ibis\n",
        "import tqdm\n",
        "import requests\n",
        "\n",
        "Metadata = dict[str, dict[str, str] | None]\n",
        "\n",
        "\n",
        "# https://github.com/ibis-project/ibis/blob/9.0.0/ibis/examples/gen_registry.py#L111-L165\n",
        "def add_nycflights13_example(data_path: Path, *, metadata: Metadata) -> None:\n",
        "    filenames = [\n",
        "        \"airlines.csv\",\n",
        "        \"airports.csv\",\n",
        "        \"flights.csv.zip\",\n",
        "        \"planes.csv\",\n",
        "        \"weather.csv\",\n",
        "    ]\n",
        "\n",
        "    BASE_URL = (\n",
        "        \"https://github.com/machow/nycflights13-py/raw/master/nycflights13/data/{}\"\n",
        "    )\n",
        "\n",
        "    def download_and_convert(filename: str, *, bar: tqdm.tqdm):\n",
        "        parquet_path = data_path / f\"nycflights13_{filename.split('.')[0]}.parquet\"\n",
        "\n",
        "        if parquet_path.exists():\n",
        "            metadata[parquet_path.with_suffix(\"\").name] = {}\n",
        "            bar.update()\n",
        "            return\n",
        "\n",
        "        if not filename.endswith(\"zip\"):\n",
        "            with tempfile.TemporaryDirectory() as d:\n",
        "                con = ibis.duckdb.connect()\n",
        "                table = con.read_csv(BASE_URL.format(filename))\n",
        "                table.to_parquet(parquet_path, codec=\"zstd\")\n",
        "        else:\n",
        "            resp = requests.get(BASE_URL.format(filename))\n",
        "            resp.raise_for_status()\n",
        "            raw_bytes = resp.content\n",
        "\n",
        "            # convert to parquet\n",
        "            with tempfile.TemporaryDirectory() as d:\n",
        "                con = ibis.duckdb.connect()\n",
        "                d = Path(d)\n",
        "                all_data = d / filename\n",
        "                all_data.write_bytes(raw_bytes)\n",
        "\n",
        "                # extract the CSVs into the current temp dir and convert them to\n",
        "                # zstd-compressed Parquet files using DuckDB\n",
        "                with zipfile.ZipFile(all_data) as zf:\n",
        "                    zf.extractall(d)\n",
        "\n",
        "                parquet_path = data_path / \"nycflights13_flights.parquet\"\n",
        "                con.read_csv(d / \"flights.csv\").to_parquet(parquet_path, codec=\"zstd\")\n",
        "\n",
        "        metadata[parquet_path.with_suffix(\"\").name] = {}\n",
        "        bar.update()\n",
        "\n",
        "    bar = tqdm.tqdm(total=len(filenames))\n",
        "    with concurrent.futures.ThreadPoolExecutor() as e:\n",
        "        for fut in concurrent.futures.as_completed(\n",
        "            e.submit(download_and_convert, filename, bar=bar) for filename in filenames\n",
        "        ):\n",
        "            fut.result()\n",
        "\n",
        "\n",
        "data_path = Path(\"data\")\n",
        "data_path.mkdir(exist_ok=True)\n",
        "\n",
        "metadata = {}\n",
        "\n",
        "print(\"Downloading the nycflights13 data...\")\n",
        "add_nycflights13_example(data_path, metadata=metadata)\n",
        "\n",
        "list(metadata.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's continue by loading the data into a local PostgreSQL database!\n",
        "\n",
        "We will do this using DuckDBâ€”yes, you can do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!psql < sql/create_nycflights13.sql\n",
        "!duckdb < sql/load_nycflights13.sql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we'll confirm that our PostgreSQL database contains the tables we just loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!psql < sql/verify_nycflights13.sql"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
